---
layout: post
title: Что такое Docker и почему он крут?
tags:
- docker
- devops
---

![docker](/images/2017/09/docker.png)

Зачем этот пост?
================

Несколько раз у меня возникала ситуация, когда я пытался объяснить коллегам или знакомым айтишникам, **что** такое Docker, **зачем** он нужен и **почему** он так крут. И у меня не получалось. Не то чтобы люди были дураки, или тема запредельно сложная, но зачастую собеседник оставался в состоянии некоего смятения, а выражение его лица говорило: "Ну и что со всем этим делать?" В чем же была сложность? Я пытался объяснить три вещи, давайте разберем их по очереди.

Что?
----

С первой частью ни у кого обычно проблем не возникает. Все прекрасно улавливают, что это **`технология контейнеризации приложений...`** и дальше по методичке.

Зачем?
------

Дальше — хуже. Выслушав техническую часть о том, что Докер — это набор технологий для изоляции приложений, а еще, возможно, увидев вот эту картинку с официального сайта… ![containers](/images/2017/09/containers.png) …люди приходят к устойчивому заблуждению о том, что **`контейнеры -- это просто легковесные виртуалки`**. Все. Различия закончились, расходимся.

Самое печальное, что это заблуждение может подкрепляться предыдущим опытом использования более старых продуктов, применяющих технологию контейнеризации. Например, **`OpenVZ`** изо всех сил старается прикидываться виртуалкой, которая в свою очередь старается прикидываться физическим сервером. В итоге забрали все недостатки и никаких достоинств: оборудование не сэмулировали, зато получили неконтролируемо загрязняющуюся среду.

Почему?
-------

Налипшие во время ответа на предыдущие вопросы слои недопонимания приводят к полной неспособности понять преимущества технологии. Сложно оценить достоинства трактора, который запрягли в лошадь вместо плуга. Кобыле тяжело, приходится подталкивать сзади, обильно матерясь. А всего-то надо было убрать лошадь, от слова совсем, вообще забыть про нее, заправить трактор и завести мотор.

Ставя во главу угла привычный ход вещей, невозможно раскрыть весь потенциал технологии контейнеризации. Вместо революционного подхода к разработке, распространению и развертыванию приложений мы получаем количественный прирост к скорости запуска песочниц и уменьшение потребления памяти. Звучит захватывающе *(нет)*.

Все плохо…
----------

Не то чтобы совсем, обычно после 3-4 итераций разъяснений и практических примеров у собеседника в голове перещелкивает некий тумблер. Он понимает, что есть **другой** подход, и у него есть свои плюсы. Но чтобы по полной начать пользоваться этими преимуществами, необходимо, чтобы в сознании человека произошли некоторые позитивные изменения, которые позволят ему посмотреть на жизненный цикл приложения под новым углом. Надеюсь, эта статья поможет уменьшить количество индивидуальных итераций допиливания сознания собеседника напильником.

Меняем подход к ознакомлению
============================

**На самом деле Докер прост, как топор. Самое сложное в работе с ним — изменить привычки. Никто не любит менять их без причины и мотивации, поэтому сперва я постараюсь дать достойную причину.**

Чтобы подоходчивее рассказать о Докере, я попробую плясать от проблемы. Ведь Докер — это инструмент, призванный решать вполне определенный круг задач, на который направлены его философия и дизайн. Расписав набор проблем, возникающих в жизненном цикле ПО, я надеюсь, что мы вместе с читателем придем к пониманию того, как должно выглядеть средство, которое их решает. Этот подход должен оказаться продуктивнее, чем: *"Есть такая диковинная хренька, сейчас я расскажу вам, как ее можно пристроить"*.

Проблемы
--------

И что это за проблемы такие? Есть вопросы, которые мучают разработчиков и админов. Звучат они по-разному, так как каждый смотрит на них под своим углом, но их объединяет одно неприятное свойство. Отсутствие простых и доступных ответов на эти вопросы порождает головную боль, ссоры с коллегами и паралич экспериментаторства.

### Админские

Для админа эти вопросы звучат следующим образом:

-   откуда оно взялось?

-   в каком оно сейчас состоянии?

Если Админ:

-   не знает, откуда на этом сервере взялось данное приложение — он не знает, как развернуть его на другом

-   не знает, что изменилось на этом сервере с момента установки приложения — он не знает, что в этой помоечке является:

    -   зависимостями приложения

    -   результатами его работы

    -   левым мусором

В мире традиционных серверов и виртуальных машин есть процесс неизбежного замусоривания окружения. Далеко за примерами ходить не надо. Думаю, у каждого хоть раз возникало желание переустановить винду из-за полного непонимания, что с ней не так, как она до такого докатилась, и что нужно сделать, чтобы она вернулась в приемлемое состояние. Проще снести и поставить заново. Проблема в том, что продакшен-сервер — не лаптоп с виндой, и переустанавливать виндоус 3 раза в день *(а, как известно, каждая переустановка занимает 20 минут)* никто не даст.

В итоге Админ превращается в **`хранителя тайного знания`**, который в меру своей порядочности и скрупулезности пытается документировать процессы, происходящие на вверенных ему серверах, старается контролировать версии софта и конфиги. Остается надеяться, что завтра его не собьет автобус.

Также приходится молиться, чтобы не упал продакшен-сервер, ведь даже если все данные пользователей уцелеют, то восстановление рабочей среды приложения, а затем его развертывание на новую среду — процедура неопределенной сложности и длительности, и вообще рулетка. Мы ведь помним, что не уверены в состоянии сервера на момент сбоя, а точного и актуального описания процедуры деплоя ни у кого нет?

Админ хочет, чтобы все бралось из понятных мест, понятным образом и всегда оставалось в неизменном состоянии. Такую систему легко понять и легко автоматизировать.

### Кодерские

В мире программирования давно правят системы контроля версий, и с происхождением тех или иных вещей вопросов меньше. Но остро стоит вопрос рабочего окружения:

-   как воссоздать на ноутбуке среду, приближенную к продакшену, со всеми его серверами приложений, базами данных и балансировщиками?

-   как избежать ситуации, когда на локалхосте работает, а "у одмена там на продакшене" — нет?

Если Кодер пишет код в среде, которая только отдаленно напоминает продакшен, начиная от версий библиотек и настроек веб-сервера, заканчивая тем, что кроме веб-сервера у него ничего и нет. Каким бы хорошим кодером не был бы Админ, но в отдаленной от реальности среде он рано или поздно забудет что-то предусмотреть (например что может быть одновременно запущенно больше одной копии его приложения), и случится пыщь.

А когда случается пыщь, начинается перекидывание камней между огородами:

> Админ: Ты нахрена адрес БД захардкодил?
>
> Кодер: А как я тебе буду его динамически получать на ноуте? И вообще, ты зачем обновил похапе?!
>
> Админ: Так там четвертая цифра в версии, минорщина же, да и вообще это обновление безопасности!

В этой ситуации долго и нервно искать виноватых, пока сервис лежит. Со временем совместные усилия победят проблему, сервис опять заведется, будут найдены виноватые и случатся прочие вещи, мало относящиеся к веселью и профиту.

После такого Кодер хочет, чтобы его приложение крутилось в строго заданной среде, начиная от локалхоста и заканчивая продакшеном. Идеально, если эти два окружения полностью совпадают. С таким окружением можно работать без сюрпризов и просто писать код.

Изобретаем решение
------------------

Пострадав за всех, мы понимаем, что хватит это терпеть, нам нужен некий инструмент/технология/методология… да что угодно, главное --- чтобы это больше не повторялось. Попробуем определить критерии решения.

### Самодостаточность

Нам нужен способ создавать для приложения строго задекларированное окружение, которое нельзя испортить. Для надежности будем загребать настолько глубоко, пока снизу не постучится здравый смысл. Подходящей точкой может оказаться ядро ОС.

Сферическому "сайтику на похапе" должно быть безразлично, на каком ядре он крутится, ему важна версия языка, фрейморка, веб-сервера, может быть, наличие каких-либо сторонних библиотек и бинарников. Давайте возьмем все эти вещи: все, что нужно для работы нашего приложения до самого низкого уровня, за исключением ядра, и назовем это **`контейнером`**. Запущенный контейнер будет изолирован от хостовой операционной системы, а приложение будет видеть только заранее определенные и заботливо подложенные в контейнер зависимости. Таким образом, мы получим самодостаточную сущность, которой не важно, где ее запустили.

Я уже вижу скупую слезу радости на лице Кодера.

### Повторяемость

Будучи людьми технического склада ума, Кодер и Админ понимают, что им нужен надежный и повторяемый способ создавать контейнеры. Ведь если один член команды смог создать "золотой контейнер", который работает как надо, но воссоздать его труды ни у кого не выходит, то чем этот контейнер лучше образа виртуалки, который надо беречь, или сервера, на котором лучше ничего не трогать?

Имея простой способ описать желаемый контейнер и надежный способ согласно этому описанию собрать из одинаковых входных данных одинаковый контейнер, мы перестаем ценить контейнеры как вещь, которую нужно беречь. Зачем, если процедура идеально повторяема?

### Самодокументируемость

Наученный горьким опытом Админ знает, что любая документация имеет свойство расходиться с реальностью. Единственная документация, которая не врет — это конфиг. Если система работает согласно конфигу — он и есть источник правды о системе. К сожалению, конфиги имеют свойство быть сложночитаемыми и путанными вещами в себе.

Значит нам нужен простой конфиг, как для процесса сборки контейнера, так и для его развертывания, чтобы покрыть весь путь от коммита в репозиторий до продакшена. Если у нас будет понятный конфиг, в который сможет взглянуть человек не в курсе деталей проекта и понять, что тут происходит — разве это не документация? Которая к тому же всегда актуальна. А еще все эти процессы можно автоматизировать. Автоматизировать жизненный цикл приложения просто написывая документацию, Карл!

Админ уже с трудом может сдерживать эмоции и сидеть на месте.

### Производительность

И все это хорошо, но если сборка контейнера будет длиться часами, или даже минутами *(да-да, в суетное время живем!)*, то никому это не будет интересно. Хочется, чтобы:

-   первичная сборка шла как можно быстрее, желательно на скорости простого копирования файлов

-   процесс сборки по максимуму кэшировался, чтобы повторные сборки происходили почти мнгновенно

-   скорость запуска контейнера минимально отличалась от скорости запуска приложения на голом железе

Количество имеет свойство переходить в качество. Как 24 фотографии в секунду превращаются в кино, так и возможность почти мнгновенно ворочать контейнерами позволит открывать новые сценарии использования.

Итак, Docker!
=============

Мы успели пострадать вместе с Админом и Кодером, поэтому нам уже должны быть понятны предпосылки появления инструмента, призванного их проблемы порешать. Читатель может обвинить меня в хитрожопости за то, что я придумал проблему и накидал решение, в канву которого отлично ложится Докер. Не буду долго оправдываться, пусть отдельные моменты и были приукрашены для остроты сюжета, а для пущей драматичности — все проблемы случились одновременно. Несмотря на это, думаю, каждый в каком-то из моментов узнал себя. Если так, то вступление свою задачу выполнило.

![longread](/images/2017/09/longread.jpg)

Этот пост по современным меркам уже трижды успел стать лонгридом, а я еще даже не начал отвечать на вопрос, поставленный в заголовке. Думаю пора начинать.

Не буду утомлять читателя деталями того, за счет каких механизмов осуществляется изоляция контейнеров. Сойдемся на том, что как-то она за кулисами работает. Главное, что надежно и предсказуемо. Есть много умных дядек, которые это расписали лучше меня. Давайте остановимся на видимых вещах, обойти которые скорее всего не получится, а наоборот — придется понять и проникнуться.

Образы, контейнеры и Dockerfile
-------------------------------

Допустим, мы решились опробовать Докер, установили его и готовы отправиться в бой. Хочется создать контейнер и запустить его! И даже нашелся в закромах какой-то "сайтик на похапе" со студенческих времен!

Прежде чем начать заворачивать его в Докер, давайте устаканим терминологию:

**`Image`**, он же образ — с него начинается Докер. Это образец, из которого будет создаваться контейнер.

**`Container`** — то, ради чего мы все тут собрались. Самодостаточные изолированные сущности, запущенные из образа.

Чтобы лучше уяснить разницу между образом и контейнером, проведем аналогию. Пусть образ — это бинарник некоторой программы, он когда-то был скомпилирован и лежит на жестком диске. Тогда контейнером будет процесс, созданный при запуске этого бинарника.

Хоть одна сущность и получилась из другой почти полным копированием, между ними есть большая разница:

-   бинарник — это просто файл на диске. Его атрибуты:

    -   размер

    -   владелец

-   процесс — это динамическая сущность, размещенная в памяти. Его атрибуты:

    -   PID

    -   владелец

    -   родитель

    -   адресное пространство и т.д.

К тому же, обычно одну и ту же программу можно запустить с различными параметрами, будь то конфиг или аргументы, и получить различный результат. Так же мы можем одновременно запустить несколько процессов из одного бинарника, но вот процесс, являющийся плодом нескольких бинарников — это какая-то дурь.

Примерно в таком же отношении состоят образ и контейнер. Я всю дорогу буду использовать аналогию **"контейнер — это процесс, а не виртуалка"** потому, что она очень хорошо вышибает вредную ассоциацию с виртуалками и помогает понять, почему некоторые вещи делаются так, а не иначе.

Мы создаем образ, который описываем в специальном файле **`Dockerfile`**. Так он скорее всего будет выглядеть для нашего сайтика:

    FROM php:5-apache
    RUN  a2enmod rewrite headers expires
    RUN  docker-php-ext-install mysql mysqli pdo pdo_mysql

    COPY src /var/www/html/

Пока не надо вдаваться в детали происходящего, читать его нужно так:

    FROM <взять какой-то образ за основу>
    RUN  <что-то доустановить>
    RUN  <что-то подкрутить>
    COPY <закинуть наш код внутрь>

Абсолютное большинство Докерфайлов будут следовать этому шаблону.

Мы берем за основу некий базовый образ. Это может быть как "низкоуровневый" образ ОС, навроде `debian`, `ubuntu`, `centos` или `alpine`, так и более специализированный образ какого-либо приложения (`nginx`, `mysql`, `jekyll`) или языка программирования (сами догадаетесь).

После этого мы его некоторым образом допиливаем до нужной кондиции:

-   доустанавливаем нужные пакеты

-   правим дефолтные конфиги

-   **пляшем с бубном**

В общем, мы почти никак не ограничены синтаксисом Докерфайла. После команды `RUN` идет обычная команда, которую вы бы написали руками в терминале. А в линуксе, как минимум в серверном, все можно сделать через терминал.

Когда окружение нас устраивает, мы закидываем в него наш код --- единственную цель существования данного образа. На этом моменте стоит остановиться поподробнее. В старом архаичном мире физических серверов и виртуалок мы привыкли к тому, что мы настраиваем среду, а уже потом устанавливаем в нее наше приложение. Эти процессы могут идти сразу друг за другом, но все-таки они разделимы. Мы интуитивно тянемся к варианту *"запустить нулевый контейнер с веб-сервером и закинуть внутрь него наш код"*, ведь это то, что мы так любим: запах свежеустановленной винды, чистый горизонт и новое начало. Но этот вариант не канает, совсем. Это мышление в терминах инфраструктуры. В мире контейнеров необходимо мыслить в терминах приложений. Код или любая другая полезная нагрузка должна быть заложена в образ на этапе сборки.

Создание образа — это, по сути, та же компиляция бинарника. Мы хотим, чтобы после сборки он мог запуститься и работать. Если создать образ без кода, то запущенный из этого образа контейнер — это мертворожденная сущность, не способная нести пользу.

Если мы напишем программу, которая умеет только висеть в процессах и ничего не делать, то мы не пытаемся открыть память этого процесса в HEX-редакторе и дописать нужный функционал в бинарном коде. Мы дописываем исходный код, пересобираем бинарник и перезапускаем процесс.

Таким образом, с помощью Докерфайла мы превращаем наш код в образ. Аналогия с `Makefile` налицо: код, инструкция по сборке и бинарник на выходе.

![dockerfile](/images/2017/09/dockerfile.png)

Прежде чем перейти к следующей теме, хочу обратить внимание на один важный момент. `Dockerfile` — практически единственный способ создать образ (за исключением извращений с Docker API). Мы не можем просто неконтролируемо накидать каких-то файлов в папочку, наклепать файл с метаданными, пожать это tar'ом и потом никому не рассказывать о том, как мы это сделали. Докер заставляет нас **документировать** процесс создания образа. `Dockerfile` является инструкцией по сборке образа, а значит — источником правды о нем. В то же время он достаточно легко читаем, чтобы служить в роли документации. Я видел не одну сотню Докерфайлов различных проектов. Скажу даже больше: я всегда в первую очередь читаю именно его, если он присутствует в репозитории, т.к. этот файл конкурирует разве что с `README.md` по информативности. Беглого взгляда хватает, чтобы понять из чего сделан этот проект.

UnionFS
-------

Я так долго и старательно повторял слово **бинарник**, пытаясь увести ассоциации читателя от виртуальных машин, что могло создаться ощущение, что образ - это и правда некий одинокий файл, который "запускается в процесс". Но эти представления становятся не так состоятельны при виде процесса загрузки образа:

    $ docker pull mysql:latest
    latest: Pulling from library/mysql
    6a5a5368e0c2: Downloading [==========>                    ] 14.68 MB/51.35 MB
    0689904e86f0: Download complete
    486087a8071d: Download complete
    3eff318f6785: Download complete
    3df41d8a4cfb: Download complete
    1b4a00485931: Download complete
    0bab0b2c2630: Download complete
    264fc9ce512d: Downloading [=>                             ] 1.621 MB/70.8 MB
    e0181dcdbbe8: Waiting
    53b082fa47c7: Waiting
    e5cf4fe00c4c: Waiting

Вместо одного прогрессбара у нас их целый вагон. У пытливого читателя будет куча вопросов:

-   Что он качает?

-   Получается образ сделан из нескольких файлов?

-   Если так, то почему они названы не по-человечески, а какими то айдишниками?

На самом деле Docker-образ реализован в виде особой файловой системы UnionFS. Ее основная идея в слоях. Суть ее в возможности зафиксировать текущее состояние файловой системы в ридонли слой, а все изменения писать в следующий, который накладывается поверх этого. Эту операцию можно повторять много раз.

По этому же принципу работает процедура инкрементальных бекапов. Мы сделали полный бекап в понедельник ночью и каждую следующую ночь делали инкрементальный бекап — то есть только разницу за сутки вместо полной резервной копии. Если у нас в пятницу все сломается, то чтобы восстановить данные, нам будет необходимо раскатать полный бекап, сделанный в начале недели, а затем последовательно накатить на него бекапы, сделанные в последующие три дня: **понедельник** ⇐ вторник ⇐ среда ⇐ четверг.

Еще одна область, где можно наблюдать похожую картину — это системы контроля версий, например Git. Мы фиксируем набор изменений в коммит, все последующие изменения отсчитываются от него и следующий коммит будет разницей между текущим состоянием и предыдущим. Еще одно сходство между гитом и Докер-образами в том, что слои и коммиты идентифицируются контрольной суммой данных, которые в них содержатся:

    $ git log --oneline
    7fc81f5 remove manual anchors
    114f6ec switch from Markdown to AsciiDoc
    5a0b5fb permalink
    8c19226 fix typos
    21dff8f UnionFS
    e1749fc some fixes
    e64fbe5 remove toc for now
    8e07398 dockerize

Набор действий, зафиксированных в виде последовательности хешей с комментариями. Похоже, не правда ли?

В Docker-образе *"коммитом"* является результат выполнения инструкции из `Dockerfile`. Если вернуться к Докерфайлу из прошлого примера,

    FROM php:5-apache
    RUN  a2enmod rewrite headers expires
    RUN  docker-php-ext-install mysql mysqli pdo pdo_mysql

    COPY src /var/www/html/

то можно понять, что в получившемся образе будет на три слоя больше, чем в родительском `php:5-apache`, т.к. мы выполнили три дополнительных инструкции.

С идеей слоеной файловой системы вроде бы разобрались, но как и во многих других аспектах Докера — понятно как, но непонятно зачем. Если в бекапах мы экономили время и место, сохраняя только разницу, а в гите мы получали возможность гулять по истории и ветвиться, то чем же так полезна слоеная ФС в случае контейнеров?

На самом деле это один из краеугольных камней архитектуры Докера, который решает кучу проблем и несет вагон плюшек.

### Производительность

Прирост производительности проявляется сразу в нескольких не связанных друг с другом местах.

Во-первых, это скорость запуска контейнера. Если образ является строго ридонли сущностью, то файловая система контейнера может, и скорее всего будет изменяться, иначе большинство приложений не смогли бы нормально работать. Что нам в таком случае делать? Создавать полную копию файловой системы образа для каждого контейнера? А вот и незачем! Мы можем просто создать новый слой, в который будут писаться изменения относительно "эталонной" ФС образа. Из-за того что при старте контейнера его ФС идеально повторяет эталон, его слой с изменениями будет нулевого размера. Угадайте, сколько времени занимает создание этого слоя ;) Но что круче всего — это правило работает и для ста контейнеров, запущенных из одного образа.

Во-вторых, существенно увеличивается скорость сборки контейнеров, иногда даже не только повторной, но об этом позже. Большинство адекватных систем сборок для различных языков и фреймворков имеют возможность кэшировать результаты сборки для того, чтобы пропускать шаги для файлов, не претерпевших изменений. Глупо было не сделать подобную систему для среды, которая собирает целые операционки за вычетом ядра. Билд-кэш в Докере работает достаточно просто. Вместо отслеживания дерева зависимостей между исходниками и различных условий, дополнительно описанных в системе сборки, логику билд-кэша в Докере можно описать одним предложением: **"Если предыдущий слой взят из кэша, а входные данные для текущего слоя не изменились — то его тоже можно взять из кэша"**.

Входные же данные могут поменяться только в двух случаях:

-   переписана инструкция в Докерфайле

-   изменились файлы, забираемые инструкциями `COPY` и `ADD`

Таким образом, написанный по best practice'ам Dockerfile почти всегда будет собирать образ практически целиком из кэша.

### Реюзабельность

Да простят меня читатели за такой оголтелый англицизм *(да, до этого было еще терпимо)*, но хорошего емкого слова в великом и могучем вспомнить не смог. Задолго до того, как я родился, умные люди поняли, что кирпичики информационных систем надо создавать так, чтобы их можно было пристроить не только в том месте, для которого они изначально создавались, но и в совершенно неожиданных местах, о которых создатели кирпичиков могли и не подозревать. Благодаря этой идеологии появились библиотеки, фреймворки и прочий UNIX-way. Мы уже видели директиву `FROM` в Докерфайле. Мы всегда можем взять за основу образ созданный до нас. Причем *базовые* образы не являются какими-то волшебными, мы просто берем набор слоев и продолжаем дописывать свои слои с того места, где закончил создатель базового образа. Вышеописанный образ сайтика тоже можно использовать в качестве базового, хотя полезность этой затеи сомнительна :)

Как и в предыдущем пункте, данная полезность всплывает в различных ситуациях. С наследованием базовых образов в Докерфайле мы уже встречались. А что со скачиванием собранных образов? Все точно так же. У меня уже был скачан образ `alpine` — минималистичного Linux-дистрибутива размером всего 5 МБ. Идеален для всяких `Hello, World!`-штукенций. А вот что произойдет, если я попытаюсь скачать образ `redis`, основанный на `alpine`:

    $ docker pull redis:alpine
    alpine: Pulling from library/redis
    c0cb142e4345: Already exists
    21f236a13876: Pull complete
    9f8988aa9114: Pull complete
    e9ff15af8577: Pull complete
    1230b63e5666: Pull complete
    2ca4ef2df2de: Pull complete
    Digest: sha256:99105b7a83dd67a0b4a86ca5f64335801c62d4f3b685eebd4fb66fdb87c66b7b
    Status: Downloaded newer image for redis:alpine

Обратите внимание на статус первого слоя: `Already exists`. Докер "понял", что у него уже есть базовый образ и качать его не нужно. Причем со свойственной ему простотой. Он не парсил какое-то дерево зависимостей и не совершал прочего сложного матана, а просто сравнил список хешей слоев из которых состоит загружаемый образ с теми, что у него уже имеются в библиотеке и все понял.

Есть и третий момент, где проявляется свойство реюзабельности слоев. В упоминавшемся ранее билд-кэше. Как мы уже успели понять, Докеру не важно, откуда появился и каким образом был создан слой, все, что важно — это его контрольная сума. И это свойство естественным образом рождает билд-кэш общий для всех проектов, собираемых на данной машине. Как именно это происходит? Представим, что мы разрабатываем несколько сайтиков. И не мудрствуя лукаво мы решаем, что будем запускать их в одинаковом окружении: какой-нибудь абстрактный "дебиан-пхп-апач". Различаться контейнеры будут только положенным внутрь кодом и, может быть, конфигами веб-сервера (что в общем объеме от всего окружения — минорщина). После того, как мы соберем образ первого проекта, все его слои осядут в кэше. Для этих проектов скорее всего будут использоваться очень похожие Докерфайлы. Значит первые совпадающие и, как правило, самые тяжеловесные директивы будут браться из кэша, даже при первой сборке "соседнего" проекта. Кэш глобален для всей машины, поэтому неважно, из-под каких пользователей собираются различные проекты. Это в полный рост сияет на CI-серверах, где никак не связанные друг с другом проекты, сделанные на популярных стеках технологий, могут создавать билд-кэши друг для друга. И никто этого специально не организовывал, оно само так получилось, из свойств слоев.

Отличной демонстрацией этому путаному объяснению может послужить графическое представление использования общих слоев в официальных базовых образах для `ruby`, `python`, `node`, `golang`, `java` и `php`.

![layers](/images/2017/09/layers.png)

Если у вас уже был скачан образ `ruby`, то при загрузке `node` мы сэкономим 125+44+122+314=605 МБ.

Рычаги влияния на контейнер
---------------------------

Мы выяснили, что образ и контейнер, это не монолитные файлы. Вернемся к нашей аналогии **`контейнер -- это бинарник`**. Любая адекватная программа дает пользователю возможность повлиять на собственное поведение. Обычно на работу программы влияет три вещи:

-   исходный код

-   конфиг

-   внешняя среда

В попытке изменить поведение программы мы реже всего трогаем ее код и пересобираем бинарник заново. Обычно мы меняем конфиг программы, будь то аргументы конмандной строки или текстовый конфигурационный файл.

Возьмем для примера такую простую программу, как `ping`. Если мы изменим ее код так, что она будет выводить `Hello, World!` и пересоберем, мы ее безнадежно испортим. Она будет делать ровно то, что мы от нее хотели. Основательно, надежно, но неудобно.

Второй способ изменить поведение программы — передать ей аргументы и обработать их в коде.

    $ ping google.com
    PING google.com (212.109.17.249) 56(84) bytes of data.
    64 bytes from cache.google.com (212.109.17.249): icmp_seq=1 ttl=60 time=1.62 ms

    $ ping some-weird-unknown-domain.com
    ping: unknown host some-weird-unknown-domain.com

Разные аргументы — разный результат, все очевидно. Не зря это самый популярный способ кастомизации поведения программ.

Внешняя среда — это то, что нам зачастую неподконтрольно, поэтому упоминаю вскользь. Но не стоит ее сбрасывать со счетов, она часто является причиной необъяснимых проблем, когда **"ниче не менял, оно само сломалося"**. Сегодня у меня Гугл пингуется, а завтра может перестать из-за аварии у провайдера, хоть параметров я и не менял.

Это все было про традиционные программы, но что это означает в случае Докера? Ну, раз "исходный код" == "контекст сборки" *(каталог проекта, в котором лежит `Dockerfile`)*, то тут никаких неожиданных особенностей. Изменили что-то в проекте — пересобрали образ.

Интереснее дела обстоят с конфигом. Как мы можем повлиять на запуск контейнера? Размер `man docker run` говорит, что много как. Чтобы не потеряться во всем многообразии опций из разряда "тонкой настройки", мы пробежимся по главным параметрам, которые потребуются при запуске чего угодно сложнее `docker run hello-wolrd`.

Итак, для запускаемого контейнера мы можем определить:

-   переменные окружения

-   публикуемые порты

-   монтируемые разделы

Это три основных аспекта, определяющих поведение нашего контейнера, пройдемся по ним по порядку.

### Переменные окружения

Переменные окружения — это предпочтительный способ передать в контейнер некие переменные. По сути это аналог параметров командной строки для классического приложения. Например, официальный образ `mysql` воспринимает переменную `MYSQL_ROOT_PASSWORD`. Думаю название говорит само за себя. При запуске контейнера скрипт инициализации выставит соответствующий рутовый пароль.

    $ docker run -d -e MYSQL_ROOT_PASSWORD=123 --name my-mysql-container mysql
    bdb010a08d2179441832896961114851e051874672639d6b3ee77922acc67540

    $ docker exec -it my-mysql-container mysql -p
    Enter password:

    <<копирайт вырезан за огромностью>>

    mysql>

Введя заветное "раз-два-три" я попал в рутовую консоль СУБД. Таким образом можно кастомизировать много аспектов работы контейнера, которые не имеет смысла хардкодить в момент сборки образа. В хорошей документации к образу всегда есть список переменных, на которые отзывается запускаемый контейнер. Особенно это полезно, когда мы хотим минимальными усилиями синхронизировать некие параметры между несколькими контейнерами, реализующими один цельный сервис. Например определить в одном месте логин, пароль, имя базы данных, которые затем будут использовать контейнеры с бекэндом и с СУБД.

### Публикуемые порты

*TL;DR для сетевиков: Докер делает NAT и Port Forwarding для контейнеров, мы можем указать пробрасываемые порты*

При создании контейнера Докер выдает контейнеру IP-адрес из приватной сети подобно тому, как это делает ваш домашний роутер, выдавая вашей домашней утвари адреса навроде 192.168.1.100. Также он производит трансляцию адресов (NAT) при попытке установить соединение с внешним миром. Так как выданный вам адрес является приватным, то он действителен только в пределах домашней сети. При внешних запросах роутер подменяет адрес отправителя с приватного на адрес своего внешнего интерфейса, чтобы удаленный сервер понимал, куда слать ответ.

Но этого не достаточно, если нужно обеспечить доступ к контейнеру из внешнего мира. Для этого мы должны заранее указать на Докер-сервере, что если из внешнего мира поступает запрос на такой-то порт, то надо его перенаправить в такой-то контейнер.

Например, вполне типичная ситуация: `docker run -p 80:80 nginx`. Если веб-сервер находится в контейнере, мы будем перенаправлять в него HTTP запросы (с 80 порта хоста на 80 порт контейнера).

### Монтируемые разделы (Volumes)

У любой адекватной программы кроме кода есть входные и выходные данные. Первые два пункта мы уже обсудили. Мы знаем, как собрать образ и как из него запустить контейнер с определенными параметрами. Хорошо, когда у нас контейнер не имеет хранимых результатов работы. Веб-сервер с кодом приложения просто отвечает на запросы, если пренебречь логами, то от его работы не остается никаких следов, кроме улыбок довольных пользователей.

*Staleless is beautiful*

К сожалению, абсолютное большинство полезных программ в ходе своей работы обрастает данными. Базы наполняются контентом, пользователи загружают смешные картинки, все это надо сохранять. Но в идеологии Докера контейнер должен быть неизменяемой эфемерной сущностью, которая не хранит состояния. Только так мы сможем без опасений убить контейнер, скачать новую версию его образа и запустить из нее такой же контейнер, только улучшенный и обновленный. Но как же данные? Ответ на вопрос до наивности прост, если поставить его по другому.

**"Если данные нельзя хранить внутри контейнера, то… их надо хранить вне контейнера"**.

Если вернуться к аналогии с классическими программами, то ответ становится достойным КО. Никакая нормальная программа не сохраняет в своем бинарном коде результаты работы, это всегда внешние файлы. Какой аналог внешних файлов будет в случае Докера? У нас два варианта, без перевода:

-   Host Mounts

-   Volumes

**Хостовые маунты** — это прямой аналог обычного юниксового `mount`, только из хоста внутрь контейнера. Мы просто говорим Докеру "забей на UnionFS, просто сделай так, чтобы контейнер думал, что вот эта моя папка оказалась внутри контейнера по такому-то адресу". В качестве примера сделаю небольшое отступление.

Я веду этот блог с применением Докера на всех этапах: от написания статей до хостинга. Сам сайт генерируется с помощью `Jekyll` — генератора статических сайтов, способного конвертировать различные языки разметки вроде `Markdown` и `AsciiDoc` в HTML. Он может следить за каталогом проекта и при каждом изменении исходных файлов перегенерировать сайт. Я использую официальный образ `jekyll/builder`, который следит за каталогом `/srv/jekyll` внутри контейнера. Я просто запустил контейнер, подмонтировав туда свой проект:

    docker run -v ~/blog/jekyll:/srv/jekyll jekyll/builder

При каждом сохранении исходного файла контейнер перегенерирует содержимое сайта и складывает его в каталог `/srv/jekyll/_site`. Я сразу же могу наблюдать его на хосте в `~/blog/jekyll/_site`, ведь операция монтирования — двунаправленная операция.

Так я по сути передаю в контейнер свои исходники и получаю из него готовый сайт. Пачка HTML-лек это, конечно, хорошо, но было бы интересно их посмотреть. Вот бы найти высокопроизводительный веб-сервер, заточенный на показ статического контента… Погодите, я слышал про такой, `nginx` зовется!

Раз уж мы все на той же волне:

    docker run -v ~/blog/jekyll/_site:/usr/share/nginx/html -p 80:80 nginx

Подмонтируем выхлоп `Jekyll` на вход `Nginx` и получим наш сайтик, открывающийся на локалхосте. Да мы же только что изобрели UNIX-way 2.0! С хостовыми маунтами и изоляцией!

*Позже я собираю образ своего бложика, где вместо монтирования по-честному кладу код сайта в базовый образ `nginx`, а затем запускаю его в Амазоне (AWS Free Tier — выбор настоящих нищебродов).*

**Разделы (Volumes)** — способ добавить больше абстракции, а значит — и масштабируемости, проблеме хранения данных в мире контейнеров. *"А что не так с хостовыми маунтами?"*. На самом деле проблем много:

-   **проблема с правами**: если контейнер создает файлы с абстрактным UID 500, то и на хосте они будут с таким же владельцем, что скорее всего будет отличаться от UID пользователя, запустившего контейнер. Если у этого пользователя нет рутовых прав — то он не сможет удалить каталог, который сам же смонтировал в контейнер.

-   **проблема с масштабируемостью**: Докер уже имеет встроенное средство создания кластера из Докер-хостов и оркестрации контейнеров. Как известно, облака — штуки волатильные. Сейчас наш контейнер разместился на сервере №5, а после перезапуска окажется на сервере №9. Причин этому может быть много, да и не должно нас это парить. В этом и состоит идея облаков — абстрагироваться от физических деталей и представить архитектуру в виде одного большого ресурса. Но что произойдет при таком перезапуске с данными, которые были сгенерированы за время работы на сервере №5? Они там и останутся! И новый контейнер на сервере №9 об этом никак не узнает, для него это будет свежий старт. Как следует из названия "Host Mounts", данные прибиты к этому хосту гвоздями и никуда не сдвинутся.

Как можно решить эти проблемы? Добавив слой абстракции, конечно же! Какую еще проблему в IT не сумели так решить? Как именно будем абстрагироваться? Дадим некоему логически изолированному куску данных имя и будем хранить их… **где-нибудь**. Не все так страшно, как звучит. За абстрактным именем будет стоять **Volume Driver** — драйвер, который разруливает, где и как хранить наши данные. В самом тривиальном случае это драйвер "local", он просто складывает наши разделы в `/var/lib/docker/volumes`. Такое простое решение закрывает вопрос с правами. Если у пользователя есть право запускать контейнеры (он состоит в группе "docker"), то у него есть право удалить раздел через `docker volume rm`.

Но, как следует из названия драйвера, данные по-прежнему остаются локальными для данного сервера. Для облака такой вариант не подходит. Для решения этой проблемы сейчас чуть ли не каждый крупный облачный провайдер (навроде Azure или Amazon) написал свои драйверы, которым только и надо что указать Access Token от аккаунта при создании раздела. Есть драйверы и для открытых распределенных ФС, не привязанных к конкретным облачным платформам. После создания раздела дальнейшая работа почти никак не отличается. Мы просто заменяем опцию `-v /path/on/host:/path/in/container` на `-v volume-name:/path/in/container`. А дальше драйвер уже как-нибудь сам разберется, откуда подтянуть данные.

Оркестрация контейнеров
=======================

В завершение хотелось бы рассказать об обратной стороне контейнеризации. Всем своим дизайном Докер толкает нас к использованию архитектуры **микросервисов**. Суть этой идеи в дроблении большого монолитного приложения на изолированные компоненты, которые общаются друг с другом через API. Такой подход позволяет минимизировать сильную связность между компонентами и улучшить их взаимозаменяемость.

Простейшим примером такого подхода будет ситуация, когда веб-разработчик захочет сделать простенький сайт, использующий базу данных. Ему понадобится как минимум два контейнера: веб-сервер и СУБД.

Проблема
--------

Даже в такой простой ситуации разработчику понадобится:

-   следить за тем, что у него запущены оба контейнера, в правльном порядке

-   следить за тем, что они запущены с правильными параметрами (порты, переменные окружения, разделы)

-   назначить им имена, которые он будет указывать в параметрах подключения

-   и еще много всяких мелочей…

Даже для двух контейнеров у нас уже будет несколько `docker`-комнад с портянкой аргументов, которые необходимо запустить в определенном порядке. Эти команды надо либо куда-то записать, либо надеяться, что `.bash_history` никуда не пропадет. А теперь представим себе более сложное приложение, состоящее из, например, 5 контейнеров. И между ними есть зависимости, возможно не только по очередности запуска. А некоторые контейнеры должны смотреть в сеть балансировщика… Список можно продолжать долго, но уже становится понятно, что ворочать этими контейнерами вручную становится нецелесообразно.

**"Тайное знание"** перемещается из области создания работающего окружения для нашего кода *(это мы порешали и одновременно задокументировали с помощью `Dockerfile`)* в область правильного объединения этих работающих окружений в один рабочий механизм.

Нам нужен способ управлять приложением как единой сущностью, не опускаясь на уровень отдельных контейнеров. Этот процесс и называется **оркестрацией**.

Варианты решений
----------------

Можно было бы по привычке накидать bash-скрипт, но у него есть ряд проблем:

-   это не самый удобочитаемый язык

-   этот подход не кроссплатформенный

-   самописные скрипты плохо переносятся на другие проекты

-   это язык императивный, мы описываем, **как** мы хотим что-либо сделать

В этом списке проблемы расположены по возрастанию их серьезности. Если с первыми двумя пунктами все более-менее понятно и, в то же время можно как-то ужиться, то по поводу последних двух надо дать пояснение.

**Чем плохи самописные скрипты?** Тем, что в них слишком много индивидуальности автора. Если какой-нибудь конфиг сложно написать как-то сильно по-другому, то в скриптовом языке у автора есть полная свобода. И каждый этой свободой пользуется в меру собственной испорченности. Это может быть как синтаксический выпендреж, так и быдлокод; как узкоспециализированный скрипт, где все захардкожено, так и попытка написать инструмент на все случаи жизни. В общем, слишком много чести просто еще одному кирпичику в архитектуре.

**Что не так с императивным языком?** В нем слишком много возможностей и подробностей. Слишком много внимания уделено тому, **как** решить задачу. Теряется ясное описание желаемого состояния системы. Описание процесса развертывания приложения в чистом поле будет выглядеть относительно просто. Кнопка "выкл" тоже не вызовет сложности. Но что делать, если система находится в состоянии "серединка-на-половинку" от желаемого? Сколько различных проверок придется описать в коде или надеяться на то, что выполненная без надобности команда ничего не поломает?

Будущее решение должно обладать следующими свойствами:

-   быть узкоспециализированным

-   использовать декларативное описание желаемого состояния системы

-   быть платформо-независимым

Решение должо уметь оценивать состояние системы, определять действия, необходимые для перехода (или возрвата) в желаемое состояние. Оно должно уметь только выполнять операции с контейнерами и связанными с ними сущностями (сетями, разделами). Не более. Уметь варить кофе ему не обязательно.

Docker Compose
--------------

![compose-logo](/images/2017/09/compose-logo.png)

В свойственной мне манере я подогнал проблему и критерии решения под существующий инструмент *(нет)*.

Docker Compose (далее Компоуз) — инструмент для управления набором контейнеров как единым целым путем декларативного описания:

-   параметров запуска контейнеров

-   зависимостей между ними

-   дополнительных сущностей (разделов, сетей)

Рассказывая про разделы, я упомянул о процессе написания этого блога. Как вы наверное успели догадаться, я не запускаю нужные контейнеры вручную, а описываю их с помощью Компоуза.

### Синтаксис

Описание приложения происходит в файле **`docker-compose.yml`**, для описания используется язык разметки `YAML`. Для моей рабочей среды он выглядит следущим образом:

    version: '2'
    services:

      builder:
        restart: always
        image: jekyll/builder
        volumes:
          - ./jekyll:/srv/jekyll


      web:
        restart: always
        image: nginx:alpine
        depends_on:
          - builder
        ports:
          - "80:80"
          - "443:443"
        volumes:
          - ./jekyll/_site:/usr/share/nginx/html

Прелесть YAML в том, что он интуитивно понятен. Если приблизительно знать, что описывает этот файл, то трудностей с пониманием синтаксиса возникнуть не должно. Небольшая справка, для тех, кто не знаком с YAML:

-   Вложенность элементов определяется отступами, поэтому важно сохранять правильную табуляцию элементов относительно друг друга. Размер табуляции, как и пробелы против табов, значения не имеют.

-   Все элементы — это пары "ключ-значение".

-   В качестве значения могут быть:

    -   единичные значения. (`image: jekyll/builder`)

    -   списки — перечисление однородных элементов. С новой строки, обычно через знак минус, хотя он может и опускаться. Пример — список портов у контейнера `web`.

    -   словари — структурированные элементы, которые могут содержать в себе элементы всех трех типов, в том числе и вложенные словари.

*Вообще весь YAML-файл — это список словарей.*

На самом верхнем уровне (без табуляции) определяются одни из следующих элементов:

-   version — версия Компоуз-файла. С версии докера 1.10 появились обратно-несовестимые нововведения, они поддерживаются только в версии 2.

-   services — главная часть, определяющая список контейнеров, составляющих наше приложение.

-   volumes — список разделов. Если мы хотим подключить к нашему контейнеру именованный раздел, то мы должны определить его в этой секции. (только версия 2)

-   networks — список сетей. Все как с разделами: хотим дополнительные сети, кроме `default` — определяем их здесь. (только версия 2)

Итого из четырех основных элементов нам в первую очередь интесен только второй. Версию всегда не думая указываем вторую, а разделы с сетями — это слегка продвинутый уровень.

Каждое определение контейнера в секции `services` — это, по сути, разложенная по полочкам команда `docker run`.

Например, соберем эквивалентную команду для запуска контейнера `web`, показывающего сгенерированный сайт:

    docker run --restart always -p "80:80" -p "443:443" \
     -v /home/zarbis/blog/jekyll/_site:/usr/share/nginx/html \
     --name blog_web_1 nginx:alpine`

Несколько замечаний к получившейся команде:

-   мы вынуждены указывать полный путь к каталогу на хосте

-   мы вынуждены придумывать имя контейнеру

-   в данной команде отсутствует аналог `depends_on`

Это довольно-таки простое описание контейнера, быть втрое больше этого — норма. А теперь представьте себе работу с проектом, состоящим из нескольких таких контейнеров. Даже простая возможность сложить все аргументы `docker run` в один файлик и запускать весь набор контейнеров одной командой уже покажется спасением от всех бед. Но это не единственное, чем облегчает жизнь Docker Compose.

### Работа

При запуске `docker-compose up` Компоуз ищет в текущем каталоге файл `docker-compose.yml`, читает его содержимое, проверяет текущее состояние системы и делает набор действий, необходимых для её перевода в задекларированное состояние. Например запуск в чистом поле выглядит следующим образом:

    $ docker-compose up -d
    Creating network "blog_default" with the default driver
    Creating blog_builder_1
    Creating blog_web_1

Компоуз проверил наличие сети данного приложения и не найдя создал её. То же самое он бы сделал, если бы у меня были определены разделы. Затем он запустил контейнеры в порядке, соответствующем директивам `depends_on`.

Если я внешними средствами убью контейнер `builder` и повторно запущу "Компоуз ап", то увижу следующую картину:

    $ docker rm -f blog_builder_1
    blog_builder_1
    $ docker-compose up -d
    Creating blog_builder_1
    Recreating blog_web_1

Он понял, что во-первых, один контейнер отсутствует, и его надо восстановить. Во-вторых — у него есть зависимые контейнеры, и, возможно, им необходим перезапуск для корректной работы.

Такая же картина будет, если я скачаю новую версию одного из используемых образов. Компоуз это поймет и так же пересоздаст контейнеры из обновленных образов. На этом свойстве можно вообще замутить целую систему обновления приложений простым баш-скриптом:

    #!/bin/bash

    docker ps --format='{{.Image}}' | sort -u | xargs -L1 docker pull

    appsdir=/app

    for service in $appsdir/*; do
        if [[ -f "$service/docker-compose.yml" ]]; then
            cd $service
            docker-compose up -d
        fi
    done

    docker rmi -f    $(docker images    -f "dangling=true" -q)

Если у вас на сервере есть каталог `/app`, а в нем каталоги с компоуз-файлами различных приложений, то закинув в крон этот скрипт, можно их невозбранно обновлять по ночам.

Еще одной приятной особенностью Компоуза является то, что он может "дозапустить" или "доопустить" приложение, находящееся в состоянии "серединка-на-половинку".

### Шпаргалка

Итак, нахвалил, а теперь расскажу как пользоваться.

В первую очередь надо создать файл `docker-compose.yml` в каталоге проекта. Имя каталога, в котором находится данный файл, будет взято в качестве префикса имен контейнеров, сетей и разделов, которые будет создавать Компоуз.

Основные полезные команды:

`docker-compose up` — запуск всего приложения. В варианте без аргументов Компоуз прицепится к логам всех контейнеров и не отдаст терминал обратно. Полезно для отладки, но если хочется просто запустить проект в фоне, то ключ `-d` в помощь. Создаваемые контейнеры будут иметь имена вида `projectname_servicename_number`. Сети и разделы — `projectname_[net|volume]name`.

`docker-compose down` — обратная операция. Останавливает и удаляет контейнеры, разбирает сети, описанные в компоуз-файле, в том числе и умолчальную `default`. Эта команда **НЕ** удаляет созданные разделы. Такое поведение соотетствует идее Докера о том, что жизненный цикл разделов отвязан от жизненного цикла контейнеров. Разделы должны переживать контейнеры, если только пользователь не попросит об обратном.

`docker-compose logs` — подцепляется к логам всех контейнеров. Полезно, когда контейнеры были запущены в фоне, но что-то пошло не так, и надо глянуть.

Это три основные команды, которые будут использоваться в 95% случаев. У 1 и 3 команды можно в конце дописать имя контейнера (краткое, как в компоуз файле, без префиксов и циферок), чтобы выполнить соответствующую операцию только для одного из них.

Еще несколько ситуативных команд:

`docker-compose exec <container> <command>` — выполнить команду внутри контейнера. Если выполнить `bash` или `sh` — то получится "провалиться" внутрь контейнера. По сути — замена `ssh` в мире контейнеров.

`docker-compose build` — производит сборку образа контейнера, если в его описании есть деректива `build:`. Для сборки берется Докерфайл, указанный в директиве.

`docker-compose push` — заливает собранный предыдущей командой образ в Реестр.

`docker-compose ps` — аналог `docker ps`, но выводит информацию только о контейнера текущего проекта.

Заключение
==========

В этой затянувшейся статье мы познакомились с проблемами-предпосылками появления технологии контейнеризации Docker в том виде, в каком он существует сегодня. Именно проблемы сформировали такой отходящий от традиций сценарий разработки и распространения приложений.

Попробую резюмировать то, к каким идеям и паттернам мы пришли.

Общие идеи о контейнерах:

-   код должен запускаться в изолированных контейнерах

-   контейнер должен нести в себе все необходимое для работы приложения

-   контейнер должен стремиться следовать идее "один контейнер — один процесс", или, в крайне случае "один контейнер — одна задача"

-   контейнер должен создаваться из декларативного описания

-   описание должно быть легкочитаемым, а процесс сборки — повторяемым

-   контейнеры не должны хранить в себе результатов работы, их должно быть не боязно прибить и пересоздать

-   данные должны храниться вне контейнера в разделах (volumes)

Также контейнеры меняют и организацию рабочего процесса. Теперь окружение следует за кодом. Положенные в репу с кодом `Dockerfile` и `docker-compose.yml` позволяют любому участнику получить окружение для разработки одной командой `docker-compose up`.

![compose-file](/images/2017/09/compose-file.png)

Самое замечательное во всем этом то, что это не просто средство разработчика для создания окружения, максимально приближенного к продакшену. Это способ для разработчиков и администраторов говорить на одном языке. Разработчик выдает готовый к использованию кирпичик системы, в отличие от разработки в каком-то локальном окружении, эмулирующем продакшен. Администратор же глядя на `docker-compose.yml` может быстро понять, как компоненты приложения взаимодействуют друг с другом и внешним миром на высоком уровне. Это позволит ему написать `docker-compose-prod.yml`, слегка измененную версию конфига, приспособленную под боевое окружение.

Таким образом, весь процесс превращения исходного кода в развернутое на боевом окружении приложение становится:

-   самодокументированным

-   выполнимым в одну команду

-   понятным всем участникам процесса

Думаю, мне удалось убедить читателя дать Докеру шанс. На этом хочу остановить свое повествование, дабы не раздувать и так уже неприличного размера статью. В будущих статьях я постараюсь более подробно описать жизнь среди контейнеров.
